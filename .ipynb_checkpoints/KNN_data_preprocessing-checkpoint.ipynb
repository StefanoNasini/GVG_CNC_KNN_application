{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab4fb57f",
   "metadata": {
    "id": "ab4fb57f"
   },
   "source": [
    "# Processing dataset for KNN and GVG-CNC\n",
    "We will be preprocessing monk1, monk2, monk3, balance-scale, tic-tac-toe, car_evaluation, kr-vs-kp datasets.\n",
    "In order for C++ code of GVG-CNC model to run, graph_data, weight_data and v_predefined_data need to be generated in a txt format with blank instead of comma.\n",
    "\n",
    "## This notebook follows the order :\n",
    "<a id='TableOfContent'> </a>\n",
    "1. <a href=#github> Github download </a>\n",
    "1. Dataset preparation\n",
    "    1. <a href=#Monk> Monk1, Monk2 and Monk3 </a>\n",
    "    1. <a href=#Balance> Balance scale </a>\n",
    "    1. <a href=#Tic> Tic Tac Toe </a>\n",
    "    1. <a href=#Car> Car evaluation </a>\n",
    "    1. <a href=#Krkp> Kr vs Kp </a>\n",
    "1. Dataset Processing \n",
    "    1. <a href=#DistanceCalculation> Calculating distances between rows </a>\n",
    "    1. <a href=#DatasetProcessing> Processing Datasets </a>\n",
    "    1. <a href=#MissingValueCreation> Creating missing values </a>\n",
    "    1. <a href=#WriteDownDatasets> Write down Dataset </a>\n",
    "1. Generation\n",
    "    1. <a href=#Graphfile> Graph weight and v predefined file generation </a>\n",
    "    1. <a href=#Phi> Phi file generation </a>\n",
    "    1. <a href=#Weight> Weight file generation </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HB4_w8IszyQi",
   "metadata": {
    "id": "HB4_w8IszyQi"
   },
   "source": [
    "<a id='Github'> </a>\n",
    "## Import data from github and change directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dIr5MnqqwqXY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dIr5MnqqwqXY",
    "outputId": "a2411b46-0a2f-45e3-b3bd-a302288f7df1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "Ar3MrJE1v1Hb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ar3MrJE1v1Hb",
    "outputId": "01c7366c-31a9-4743-c63e-10527ac06abc"
   },
   "outputs": [],
   "source": [
    "#----------------------\n",
    "# Clone the GITHUB directory\n",
    "#----------------------\n",
    "\n",
    "# !git clone -b master https://github.com/StefanoNasini/GVG_CNC_KNN_application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "yDJeYRJVzVIG",
   "metadata": {
    "id": "yDJeYRJVzVIG"
   },
   "outputs": [],
   "source": [
    "#----------------------\n",
    "# Set working directory\n",
    "#----------------------\n",
    "\n",
    "# os.chdir(\"GVG_CNC_KNN_application\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02927b4",
   "metadata": {
    "id": "d02927b4"
   },
   "source": [
    "<a id='Monk'> </a>\n",
    "## Data preprocessing -- Monk1 to Monk3\n",
    "<a href=#TableOfContent> Back to Table of Content </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ebb564f",
   "metadata": {
    "id": "2ebb564f"
   },
   "outputs": [],
   "source": [
    "#----------------------\n",
    "# use a dictionary to store all the dataset\n",
    "#----------------------\n",
    "\n",
    "total_dataset = dict()\n",
    "\n",
    "try:\n",
    "    root\n",
    "except NameError:\n",
    "    root = os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bgPOV6I2v_xc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bgPOV6I2v_xc",
    "outputId": "a3d7e4b6-120a-4273-bacb-1a1d9aaf1fd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Index',\n",
       " 'monks-1.test',\n",
       " 'monks-1.train',\n",
       " 'monks-2.test',\n",
       " 'monks-2.train',\n",
       " 'monks-3.test',\n",
       " 'monks-3.train',\n",
       " 'monks.names',\n",
       " 'thrun.comparison.dat',\n",
       " 'thrun.comparison.ps.Z',\n",
       " 'update']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----------------------\n",
    "# Monk direcotry\n",
    "#----------------------\n",
    "\n",
    "os.chdir(os.path.join(root, \"monk\"))\n",
    "\n",
    "# Inspect existing files\n",
    "\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5de1dbe1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "5de1dbe1",
    "outputId": "5a94ed65-1c12-4990-cd12-ed474bb44e48"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>1.1</th>\n",
       "      <th>1.2</th>\n",
       "      <th>1.3</th>\n",
       "      <th>2</th>\n",
       "      <th>2.1</th>\n",
       "      <th>data_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>data_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>data_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>data_10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>data_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>data_13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  0  1  1.1  1.2  1.3  2  2.1   data_4\n",
       "0         NaN  0  1    1    1    1  4    1   data_7\n",
       "1         NaN  0  1    1    1    2  1    1   data_9\n",
       "2         NaN  0  1    1    1    2  1    2  data_10\n",
       "3         NaN  0  1    1    1    2  2    1  data_11\n",
       "4         NaN  0  1    1    1    2  3    1  data_13"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----------------------\n",
    "# explore data structure\n",
    "#----------------------\n",
    "\n",
    "df = pd.read_csv('monks-2.train', delimiter=' ')\n",
    "df.head()\n",
    "\n",
    "# need to remove the first and last column\n",
    "# column \"1\" is label, 1.1 to 1.5 are features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c73d35a",
   "metadata": {
    "id": "7c73d35a"
   },
   "outputs": [],
   "source": [
    "#----------------------\n",
    "# read all monk datasets\n",
    "#----------------------\n",
    "\n",
    "names = [ 'monks-1.test',\n",
    " 'monks-1.train',\n",
    " 'monks-2.test',\n",
    " 'monks-2.train',\n",
    " 'monks-3.test',\n",
    " 'monks-3.train']\n",
    "\n",
    "# create a dictionary to store all the monk datasets\n",
    "\n",
    "monk_dict = dict(zip(names, names))\n",
    "\n",
    "# rename the variable from 'a' to 'f', then define a1-a5, ..., f1-f5 as dummy variables\n",
    "\n",
    "for name in names:\n",
    "    df = pd.read_csv(name, delimiter=' ')\n",
    "    df = df.iloc[:, range(1, 8)]\n",
    "    df.columns= ['label', 'a', 'b', 'c', 'd', 'e', 'f']\n",
    "    df['a1'] = df['a'] == 1\n",
    "    df['a2'] = df['a'] == 2\n",
    "    df['a3'] = df['a'] == 3\n",
    "    df['a4'] = df['a'] == 4\n",
    "    df['a5'] = df['a'] == 5\n",
    "\n",
    "    df['b1'] = df['b'] == 1\n",
    "    df['b2'] = df['b'] == 2\n",
    "    df['b3'] = df['b'] == 3\n",
    "    df['b4'] = df['b'] == 4\n",
    "    df['b5'] = df['b'] == 5\n",
    "\n",
    "    df['c1'] = df['c'] == 1\n",
    "    df['c2'] = df['c'] == 2\n",
    "    df['c3'] = df['c'] == 3\n",
    "    df['c4'] = df['c'] == 4\n",
    "    df['c5'] = df['c'] == 5\n",
    "\n",
    "    df['d1'] = df['d'] == 1\n",
    "    df['d2'] = df['d'] == 2\n",
    "    df['d3'] = df['d'] == 3\n",
    "    df['d4'] = df['d'] == 4\n",
    "    df['d5'] = df['d'] == 5\n",
    "\n",
    "    df['e1'] = df['e'] == 1\n",
    "    df['e2'] = df['e'] == 2\n",
    "    df['e3'] = df['e'] == 3\n",
    "    df['e4'] = df['e'] == 4\n",
    "    df['e5'] = df['e'] == 5\n",
    "\n",
    "    df['f1'] = df['f'] == 1\n",
    "    df['f2'] = df['f'] == 2\n",
    "    df['f3'] = df['f'] == 3\n",
    "    df['f4'] = df['f'] == 4\n",
    "    df['f5'] = df['f'] == 5\n",
    "\n",
    "    new_columns = \"label a1 a2 a3 a4 a5 b1 b2 b3 b4 b5 c1 c2 c3 c4 c5 d1 d2 d3 d4 d5 e1 e2 e3 e4 e5 f1 f2 f3 f4 f5\".split(' ')\n",
    "    new_columns = \"label a1 a2 b1 b2 c1 d1 d2 e1 e2 e3 f1\".split(' ')\n",
    "    df = df[new_columns]\n",
    "    monk_dict[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd37ae5a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cd37ae5a",
    "outputId": "b2f3f3c8-4346-4081-b55d-f67b70462048"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monks-1.test has 431 instances and 11 features.\n",
      "monks-1.train has 123 instances and 11 features.\n",
      "monks-2.test has 431 instances and 11 features.\n",
      "monks-2.train has 168 instances and 11 features.\n",
      "monks-3.test has 431 instances and 11 features.\n",
      "monks-3.train has 121 instances and 11 features.\n"
     ]
    }
   ],
   "source": [
    "#----------------------\n",
    "# inspect the dimension of dummy-variable-ized dataset\n",
    "#----------------------\n",
    "\n",
    "for name in names:\n",
    "    n_instances, n_features_ = monk_dict[name].shape\n",
    "#     print(monk_dict[name].sum())\n",
    "    print(f\"{name} has {n_instances} instances and {n_features_-1} features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acc42545",
   "metadata": {
    "id": "acc42545"
   },
   "outputs": [],
   "source": [
    "#----------------------\n",
    "# combine the train and test dataset together\n",
    "#----------------------\n",
    "\n",
    "monk1 = pd.concat([monk_dict['monks-1.train'], monk_dict['monks-1.test']], axis=0)\n",
    "monk2 = pd.concat([monk_dict['monks-2.train'], monk_dict['monks-2.test']], axis=0)\n",
    "monk3 = pd.concat([monk_dict['monks-3.train'], monk_dict['monks-3.test']], axis=0)\n",
    "\n",
    "monk1.index= range(monk1.shape[0])\n",
    "monk2.index= range(monk2.shape[0])\n",
    "monk3.index= range(monk3.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "820ba78c",
   "metadata": {
    "id": "820ba78c"
   },
   "outputs": [],
   "source": [
    "#----------------------\n",
    "# store them into total dataset\n",
    "#----------------------\n",
    "\n",
    "total_dataset['monk1'] = monk1\n",
    "total_dataset['monk2'] = monk2\n",
    "total_dataset['monk3'] = monk3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a8ae9f",
   "metadata": {
    "id": "b6a8ae9f"
   },
   "source": [
    "<a id='Balance'> </a>\n",
    "## Data Preprocessing -- Balance_scale\n",
    "<a href=#TableOfContent> Back to Table of Content </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c22882b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c22882b4",
    "outputId": "ca6359e6-f68f-4581-d936-de592ffac705"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['balance-scale.data', 'balance-scale.names', 'Index']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----------------------\n",
    "# change directory and inspect files\n",
    "#----------------------\n",
    "\n",
    "os.chdir(os.path.join(root, \"balance_scale\"))\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e55d4dec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "e55d4dec",
    "outputId": "517ceb20-844d-4112-9e91-6dcd5ccd10a3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B</th>\n",
       "      <th>1</th>\n",
       "      <th>1.1</th>\n",
       "      <th>1.2</th>\n",
       "      <th>1.3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   B  1  1.1  1.2  1.3\n",
       "0  R  1    1    1    2\n",
       "1  R  1    1    1    3\n",
       "2  R  1    1    1    4\n",
       "3  R  1    1    1    5\n",
       "4  R  1    1    2    1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect data structure\n",
    "\n",
    "balance_scale = pd.read_csv(\"balance-scale.data\")\n",
    "balance_scale.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0ea32a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0ea32a8",
    "outputId": "9cdbd80e-913c-4191-e916-b6853748116d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label ['R' 'L' 'B']\n",
      "a [1 2 3 4 5]\n",
      "b [1 2 3 4 5]\n",
      "c [1 2 3 4 5]\n",
      "d [2 3 4 5 1]\n"
     ]
    }
   ],
   "source": [
    "# observe possible values for each variable\n",
    "df = balance_scale\n",
    "df.columns=['label', 'a', 'b', 'c', 'd']\n",
    "for col in df.columns:\n",
    "    print(col, df[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9eff0aa0",
   "metadata": {
    "id": "9eff0aa0"
   },
   "outputs": [],
   "source": [
    "# create dummy variables named a1-a5, ..., d1-d5\n",
    "\n",
    "df['a1'] = df['a'] == 1\n",
    "df['a2'] = df['a'] == 2\n",
    "df['a3'] = df['a'] == 3\n",
    "df['a4'] = df['a'] == 4\n",
    "df['a5'] = df['a'] == 5\n",
    "\n",
    "df['b1'] = df['b'] == 1\n",
    "df['b2'] = df['b'] == 2\n",
    "df['b3'] = df['b'] == 3\n",
    "df['b4'] = df['b'] == 4\n",
    "df['b5'] = df['b'] == 5\n",
    "\n",
    "df['c1'] = df['c'] == 1\n",
    "df['c2'] = df['c'] == 2\n",
    "df['c3'] = df['c'] == 3\n",
    "df['c4'] = df['c'] == 4\n",
    "df['c5'] = df['c'] == 5\n",
    "\n",
    "df['d1'] = df['d'] == 1\n",
    "df['d2'] = df['d'] == 2\n",
    "df['d3'] = df['d'] == 3\n",
    "df['d4'] = df['d'] == 4\n",
    "df['d5'] = df['d'] == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7870c59",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7870c59",
    "outputId": "c4ecb15a-c610-48ce-aa48-2c643448a74c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B', 'L', 'R'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classes are \"Balanced\" \"Left\" and \"Right\"\n",
    "set(df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e692180d",
   "metadata": {
    "id": "e692180d"
   },
   "outputs": [],
   "source": [
    "# select only newly created columns as a new dataframe\n",
    "new_columns= \"label a1 a2 a3 a4 b1 b2 b3 b4 c1 c2 c3 c4 d1 d2 d3 d4\".split(\" \")\n",
    "df = df[new_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "482518bf",
   "metadata": {
    "id": "482518bf"
   },
   "outputs": [],
   "source": [
    "# create a dataset, where target is such that \"B\" (stands for Balanced) ->1 and other case->0\n",
    "balance_scale_B = df.replace({'B':1, 'L':0, 'R':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c15c9be6",
   "metadata": {
    "id": "c15c9be6"
   },
   "outputs": [],
   "source": [
    "# create a dataset, where target is such that \"L\" (stands for Left) ->1 and other case->0\n",
    "balance_scale_L = df.replace({'B':0, 'L':1, 'R':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65d1d9ef",
   "metadata": {
    "id": "65d1d9ef"
   },
   "outputs": [],
   "source": [
    "# store them into total dataset\n",
    "total_dataset |= dict(balance_scale_B=balance_scale_B)\n",
    "total_dataset |= dict(balance_scale_L=balance_scale_L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a64864c",
   "metadata": {
    "id": "3a64864c"
   },
   "source": [
    "<a id='Tic'> </a>\n",
    "## Data preprocessing -- tic-tac-toe\n",
    "<a href=#TableOfContent> Back to Table of Content </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2da25422",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2da25422",
    "outputId": "d731a1c3-8be6-47fb-b49e-5f3b214e8da1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Index', 'tic-tac-toe.data', 'tic-tac-toe.names']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change directory and inspect files\n",
    "os.chdir(os.path.join(root, \"tic+tac+toe+endgame\"))\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1798e1af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "1798e1af",
    "outputId": "dd8ad7f4-2781-4ed6-e150-75396a864aea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>x.1</th>\n",
       "      <th>x.2</th>\n",
       "      <th>x.3</th>\n",
       "      <th>o</th>\n",
       "      <th>o.1</th>\n",
       "      <th>x.4</th>\n",
       "      <th>o.2</th>\n",
       "      <th>o.3</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>957 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     x x.1 x.2 x.3  o o.1 x.4 o.2 o.3  positive\n",
       "0    x   x   x   x  o   o   o   x   o  positive\n",
       "1    x   x   x   x  o   o   o   o   x  positive\n",
       "2    x   x   x   x  o   o   o   b   b  positive\n",
       "3    x   x   x   x  o   o   b   o   b  positive\n",
       "4    x   x   x   x  o   o   b   b   o  positive\n",
       "..  ..  ..  ..  .. ..  ..  ..  ..  ..       ...\n",
       "952  o   x   x   x  o   o   o   x   x  negative\n",
       "953  o   x   o   x  x   o   x   o   x  negative\n",
       "954  o   x   o   x  o   x   x   o   x  negative\n",
       "955  o   x   o   o  x   x   x   o   x  negative\n",
       "956  o   o   x   x  x   o   o   x   x  negative\n",
       "\n",
       "[957 rows x 10 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read and inspect data\n",
    "df = pd.read_csv(\"tic-tac-toe.data\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2991f8e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2991f8e4",
    "outputId": "9377ba45-9302-46cf-af32-7764040e033f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b', 'o', 'x'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# give columns meaningful names\n",
    "df = df[['positive', 'x', 'x.1', 'x.2', 'x.3', 'x.4', 'o', 'o.1', 'o.2', 'o.3']]\n",
    "tic = df\n",
    "df.columns = ['label', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "# check and found out that feature is not binary\n",
    "set(df['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fd4fc6a",
   "metadata": {
    "id": "1fd4fc6a"
   },
   "outputs": [],
   "source": [
    "# transform features into binary. Knowing that each original feature represent a configuration in a cell out of 3x3 grid.\n",
    "# which can either take value 'o' or 'x' or 'nothing'.\n",
    "\n",
    "if df['label'][0] in ['positive', 'negative']:\n",
    "    df['label'] = df['label'] == 'positive'\n",
    "\n",
    "df['x1'] = df['1'] == 'x'\n",
    "df['x2'] = df['2'] == 'x'\n",
    "df['x3'] = df['3'] == 'x'\n",
    "\n",
    "df['x4'] = df['4'] == 'x'\n",
    "df['x5'] = df['5'] == 'x'\n",
    "df['x6'] = df['6'] == 'x'\n",
    "\n",
    "df['x7'] = df['7'] == 'x'\n",
    "df['x8'] = df['8'] == 'x'\n",
    "df['x9'] = df['9'] == 'x'\n",
    "\n",
    "\n",
    "df['o1'] = df['1'] == 'o'\n",
    "df['o2'] = df['2'] == 'o'\n",
    "df['o3'] = df['3'] == 'o'\n",
    "\n",
    "df['o4'] = df['4'] == 'o'\n",
    "df['o5'] = df['5'] == 'o'\n",
    "df['o6'] = df['6'] == 'o'\n",
    "\n",
    "df['o7'] = df['7'] == 'o'\n",
    "df['o8'] = df['8'] == 'o'\n",
    "df['o9'] = df['9'] == 'o'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b1749fb",
   "metadata": {
    "id": "6b1749fb"
   },
   "outputs": [],
   "source": [
    "# select newly created variable and store into total dataset\n",
    "tic_bin = df[['label', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'o1', 'o2', 'o3', 'o4', 'o5', 'o6', 'o7', 'o8', 'o9']]\n",
    "total_dataset |= dict(tic_bin=tic_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecffde4",
   "metadata": {
    "id": "1ecffde4"
   },
   "source": [
    "<a id='Car'> </a>\n",
    "## Data preprocessing -- car_evaluation\n",
    "<a href=#TableOfContent> Back to Table of Content </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "395c959b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "395c959b",
    "outputId": "3bc80ae0-4783-45c2-836e-a2515dce5c02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car.c45-names', 'car.data', 'car.names']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change directory and inspect files\n",
    "\n",
    "os.chdir(os.path.join(root, 'car+evaluation'))\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "899a581d",
   "metadata": {
    "id": "899a581d"
   },
   "outputs": [],
   "source": [
    "# read and give meaningful names to each column\n",
    "df = pd.read_csv('car.data')\n",
    "df.columns=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'label']\n",
    "df = df[['label', 'buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32ac1f1a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32ac1f1a",
    "outputId": "c0cb75a6-2d1c-4200-dca8-7ca23e5427b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label ['unacc' 'acc' 'vgood' 'good']\n",
      "buying ['vhigh' 'high' 'med' 'low']\n",
      "maint ['vhigh' 'high' 'med' 'low']\n",
      "doors ['2' '3' '4' '5more']\n",
      "persons ['2' '4' 'more']\n",
      "lug_boot ['small' 'med' 'big']\n",
      "safety ['med' 'high' 'low']\n"
     ]
    }
   ],
   "source": [
    "# See for each variable, what are the possible values.\n",
    "for col in df.columns:\n",
    "    print(col, df[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5206a575",
   "metadata": {
    "id": "5206a575"
   },
   "outputs": [],
   "source": [
    "# Give each variable numerical encodings since they are all ordinal variables\n",
    "df['label'].replace({\"unacc\":0, \"acc\":1, \"vgood\":2, \"good\":3}, inplace=True)\n",
    "df['buying'].replace({'vhigh':3, 'high':2, 'med':1, 'low':0}, inplace=True)\n",
    "df['maint'].replace({'vhigh':3, 'high':2, 'med':1, 'low':0}, inplace=True)\n",
    "df['doors'].replace({'2':2, '3':3, '4':4, '5more':5}, inplace=True)\n",
    "df['persons'].replace({'2':2, '4':4, 'more':5}, inplace=True)\n",
    "df['lug_boot'].replace({'small':0, 'med':1, 'big':2}, inplace=True)\n",
    "df['safety'].replace({'med':1, 'high':2, 'low':0}, inplace=True)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eaf8260a",
   "metadata": {
    "id": "eaf8260a"
   },
   "outputs": [],
   "source": [
    "# create binary variables based on different thresholds\n",
    "df['labelvgood'] = df['label'] == 3\n",
    "df['labelgood'] = df['label'] >= 2\n",
    "df['labelacc'] = df['label'] >= 1\n",
    "\n",
    "df['buyingvhigh'] = df['buying'] == 3\n",
    "df['buyinghigh'] = df['buying'] >= 2\n",
    "df['buyingmed'] = df['buying'] >= 1\n",
    "\n",
    "df['maintvhigh'] = df['maint'] == 3\n",
    "df['mainthigh'] = df['maint'] >= 2\n",
    "df['maintmed'] = df['maint'] >= 1\n",
    "\n",
    "df[\"doors5\"] = df['doors'] >= 5\n",
    "df[\"doors4\"] = df['doors'] >= 4\n",
    "df[\"doors3\"] = df['doors'] >= 3\n",
    "\n",
    "df[\"persons5\"] = df['persons']>=5\n",
    "df[\"persons4\"] = df['persons']>=4\n",
    "\n",
    "df[\"lug_boot2\"] = df[\"lug_boot\"] >=2\n",
    "df[\"lug_boot1\"] = df[\"lug_boot\"] >=1\n",
    "\n",
    "df[\"safety2\"] = df[\"safety\"] >=2\n",
    "df[\"safety1\"] = df[\"safety\"] >=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2340484b",
   "metadata": {
    "id": "2340484b"
   },
   "outputs": [],
   "source": [
    "# select newly created features and make 3 datasets based on target labels\n",
    "features = ['buyingvhigh', 'buyinghigh', 'buyingmed',\n",
    "            'maintvhigh', 'mainthigh', 'maintmed',\n",
    "            \"doors5\", \"doors4\", \"doors3\",\n",
    "            \"persons5\", \"persons4\",\n",
    "            \"lug_boot2\", \"lug_boot1\",\n",
    "            \"safety2\", \"safety1\"]\n",
    "car_evaluation_vgood = df[[\"labelvgood\"]+features]\n",
    "car_evaluation_good = df[[\"labelgood\"]+features]\n",
    "car_evaluation_acc = df[[\"labelacc\"]+features]\n",
    "\n",
    "car_evaluation_vgood = car_evaluation_vgood.rename(columns={\"labelvgood\": \"label\"})\n",
    "car_evaluation_good = car_evaluation_good.rename(columns={\"labelgood\": \"label\"})\n",
    "car_evaluation_acc = car_evaluation_acc.rename(columns={\"labelacc\": \"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9e24b7c",
   "metadata": {
    "id": "b9e24b7c"
   },
   "outputs": [],
   "source": [
    "# store them into total dataset\n",
    "total_dataset |= dict(car_evaluation_vgood=car_evaluation_vgood,\n",
    "                     car_evaluation_good=car_evaluation_good,\n",
    "                     car_evaluation_acc=car_evaluation_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79c25d6",
   "metadata": {
    "id": "f79c25d6"
   },
   "source": [
    "<a id='Krkp'> </a>\n",
    "## Data preprocessing -- kr-vs-kp\n",
    "<a href=#TableOfContent> Back to Table of Content </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "efd82dcf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "efd82dcf",
    "outputId": "acc71f99-f83a-4c48-dfc3-5200d53ae430"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kr-vs-kp_csv.csv']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change directory and inspect files\n",
    "os.chdir(os.path.join(root, \"kr-vs-kp\"))\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a375ddd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8a375ddd",
    "outputId": "1c36b317-638e-42ec-d258-9fdae41445ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bkblk\n",
      "['f' 't']\n",
      "bknwy\n",
      "['f' 't']\n",
      "bkon8\n",
      "['f' 't']\n",
      "bkona\n",
      "['f' 't']\n",
      "bkspr\n",
      "['f' 't']\n",
      "bkxbq\n",
      "['f' 't']\n",
      "bkxcr\n",
      "['f' 't']\n",
      "bkxwp\n",
      "['f' 't']\n",
      "blxwp\n",
      "['f' 't']\n",
      "bxqsq\n",
      "['f' 't']\n",
      "cntxt\n",
      "['f' 't']\n",
      "dsopp\n",
      "['f' 't']\n",
      "dwipd\n",
      "['l' 'g']\n",
      "hdchk\n",
      "['f' 't']\n",
      "katri\n",
      "['n' 'w' 'b']\n",
      "mulch\n",
      "['f' 't']\n",
      "qxmsq\n",
      "['f' 't']\n",
      "r2ar8\n",
      "['t' 'f']\n",
      "reskd\n",
      "['f' 't']\n",
      "reskr\n",
      "['f' 't']\n",
      "rimmx\n",
      "['f' 't']\n",
      "rkxwp\n",
      "['f' 't']\n",
      "rxmsq\n",
      "['f' 't']\n",
      "simpl\n",
      "['f' 't']\n",
      "skach\n",
      "['f' 't']\n",
      "skewr\n",
      "['t' 'f']\n",
      "skrxp\n",
      "['f' 't']\n",
      "spcop\n",
      "['f' 't']\n",
      "stlmt\n",
      "['f' 't']\n",
      "thrsk\n",
      "['f' 't']\n",
      "wkcti\n",
      "['f' 't']\n",
      "wkna8\n",
      "['f' 't']\n",
      "wknck\n",
      "['f' 't']\n",
      "wkovl\n",
      "['t' 'f']\n",
      "wkpos\n",
      "['t' 'f']\n",
      "wtoeg\n",
      "['n' 't']\n",
      "class\n",
      "['won' 'nowin']\n"
     ]
    }
   ],
   "source": [
    "# read and insepct possible values for each columns to see if they are already binary, and what are current values\n",
    "df = pd.read_csv(\"kr-vs-kp_csv.csv\")\n",
    "for col in df.columns:\n",
    "    print(col)\n",
    "    print(df[col].unique())\n",
    "\n",
    "df['katri_n'] = df['katri'] == 'n'\n",
    "df['katri_b'] = df['katri'] == 'b' # binary-ize all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be6e2106",
   "metadata": {
    "id": "be6e2106"
   },
   "outputs": [],
   "source": [
    "# reorder the columns so that the label is the first (index 0) appearing.\n",
    "new_columns = ['class', 'bkblk', 'bknwy', 'bkon8', 'bkona', 'bkspr', 'bkxbq', 'bkxcr', 'bkxwp', 'blxwp', 'bxqsq', 'cntxt', 'dsopp', 'dwipd', 'hdchk', 'mulch', 'qxmsq', 'r2ar8', 'reskd', 'reskr', 'rimmx', 'rkxwp', 'rxmsq', 'simpl', 'skach', 'skewr', 'skrxp', 'spcop', 'stlmt', 'thrsk', 'wkcti', 'wkna8', 'wknck', 'wkovl', 'wkpos', 'wtoeg', 'katri_n', 'katri_b']\n",
    "df = df[new_columns] # reorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60d58f3f",
   "metadata": {
    "id": "60d58f3f"
   },
   "outputs": [],
   "source": [
    "# rename the features to be 0 1 while making sure each column is treaty correctly\n",
    "df.replace({\"won\":1, \"nowin\":0, \"t\":0, \"f\":1}, inplace=True)\n",
    "df.replace({\"l\":0, \"g\":1, \"n\":1}, inplace=True)\n",
    "\n",
    "df.columns = ['label'] + list(df.columns)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e5f6193",
   "metadata": {
    "id": "3e5f6193"
   },
   "outputs": [],
   "source": [
    "# store it into total dataset\n",
    "total_dataset['kr-vs-kp'] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db7803",
   "metadata": {
    "id": "e1db7803"
   },
   "source": [
    "<a id='DatasetProcessing'> </a>\n",
    "# Processing Datasets\n",
    "## Getting statistics for each dataset\n",
    "<a href=#TableOfContent> Back to Table of Content </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e742d6be",
   "metadata": {
    "id": "e742d6be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'monk1': (554, 12),\n",
       " 'monk2': (599, 12),\n",
       " 'monk3': (552, 12),\n",
       " 'balance_scale_B': (624, 17),\n",
       " 'balance_scale_L': (624, 17),\n",
       " 'tic_bin': (957, 19),\n",
       " 'car_evaluation_vgood': (1727, 16),\n",
       " 'car_evaluation_good': (1727, 16),\n",
       " 'car_evaluation_acc': (1727, 16),\n",
       " 'kr-vs-kp': (3196, 38)}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimensions = dict()\n",
    "for key, df in total_dataset.items():\n",
    "    dimensions[key] = df.shape\n",
    "dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40745f0b",
   "metadata": {},
   "source": [
    "<a id='DistanceCalculation'> </a>\n",
    "## Calculating distances between rows\n",
    "<a href=#TableOfContent> Back to Table of Content </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79bc01f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the distance between instances, i.e. two given rows. Euclidien 2-distance squared is used.\n",
    "def distance(df, i, j):\n",
    "    dff = df.astype(int)\n",
    "    return sum((dff.iloc[i, 1:] - dff.iloc[j, 1:])**2)\n",
    "    \n",
    "# For the instance in i-th row, calculate the distance of it with all the other instances.\n",
    "# Reorder the distances and pick the K-first and K-last instances. If equality, pick all the instances of the same distance.\n",
    "def calculate_distances(df, i, distmatrix=None):\n",
    "    if distmatrix is not None:\n",
    "        return list(zip(range(df.shape[0]), distmatrix[i]))\n",
    "    distances = [(j, distance(df, i, j)) for j in range(df.shape[0])] # attention : (i, 0) will be inside\n",
    "    return distances\n",
    "\n",
    "def calculate_distmatrix(df):\n",
    "    distmatrix = pd.DataFrame(index=range(df.shape[0]), columns=range(df.shape[0]), dtype=np.int8)\n",
    "    distmatrix.iloc[:, :] = 0\n",
    "    for t in range(1, df.shape[1]):\n",
    "        df_t = np.array(df.iloc[:, t])\n",
    "        symmetric_diff = df_t[:, np.newaxis] ^ df_t\n",
    "        distmatrix += symmetric_diff.astype(np.int8)\n",
    "    return distmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb7a096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_d_from0_tomax(distances, K=3, training=None):\n",
    "    # create a dictionary to count : how many instances, j, are at a given distance (to the anchor instance i) ?\n",
    "    # training is an array of indices indicating which ones are training. So only training is taken into account when it \n",
    "    # comes to selecting the K nearest and farest neighbors\n",
    "    if training is None:\n",
    "        training = range(len(distances))\n",
    "#     distances.sort(key=lambda x: x[1])\n",
    "    \n",
    "    \n",
    "#     # count how many instances is of certain distance to i, the given instance.\n",
    "#     counter_df = pd.DataFrame(distances, columns=[\"index\", \"distance_to_i\"])\n",
    "#     counter = counter_df.iloc[training, 1].value_counts()\n",
    "#     counter = dict(counter)\n",
    "    counter = pd.Series(distances, dtype=np.int8).value_counts()\n",
    "    counter = dict(counter)\n",
    "\n",
    "    # find nearest neighbors\n",
    "    # pick a distance d_from0 such that (j, d) with d<d_from0 counts more than K\n",
    "    d_from0 = 0 # we pick d<d_thr\n",
    "    count = 0\n",
    "    while count < K+1: # (i, 0) are in and we want to exclude this\n",
    "        try:\n",
    "            count += counter[d_from0]\n",
    "        except KeyError:\n",
    "            pass\n",
    "        d_from0 += 1\n",
    "\n",
    "    # find farthest neighbors\n",
    "    # pick a distance d_tomax such that (j, d) with d > d_tomax counts more than K\n",
    "#     d_tomax = max(d for (j, d) in distances)\n",
    "    d_tomax = max(distances)\n",
    "    count = 0\n",
    "    while count < K:\n",
    "        try:\n",
    "            count += counter[d_tomax]\n",
    "        except KeyError:\n",
    "            pass\n",
    "        d_tomax -= 1\n",
    "    return d_from0, d_tomax\n",
    "\n",
    "def create_phidata_list(zip_distances, d_from0, d_tomax, i, draw_take_all=True, training=None):\n",
    "    # return a list, in order of index number, of whether it is among K-nearest (1), or among K-farthest(2), or neither case(0).\n",
    "    distances = zip_distances\n",
    "    # if distance is small (d < d_from0) then we select as attractive neighbor ; if big (d>d_tomax) then repulsive\n",
    "    def aux(d, d_from0, d_tomax):\n",
    "        if d < d_from0:\n",
    "            return 1 # 1 for phiup, attractive\n",
    "        elif d > d_tomax:\n",
    "            return 2 # 2 for phidown, repulsive\n",
    "        else:\n",
    "            return 0 # 0 for no link\n",
    "\n",
    "    phidata_list = list(map(lambda jd: aux(jd[1], d_from0, d_tomax), sorted(distances, key=lambda jd: jd[0]) ))\n",
    "    phidata_list[i] = 0  # you don't select yourself, i is closest to itself but not useful in this model for social influence\n",
    "    if training is None:\n",
    "        return phidata_list\n",
    "\n",
    "    # if not in training set, we shouldn't select\n",
    "    for j in range(len(distances)):\n",
    "        if j not in training:\n",
    "            phidata_list[j] = 0\n",
    "    return phidata_list\n",
    "        \n",
    "\n",
    "    \n",
    "def find_distances_and_selected_neighbors(df, i, K=3, draw_take_all=True, training=None, distances=None):\n",
    "    if training is not None and i in training:\n",
    "        return [0]*df.shape[0]\n",
    "    \n",
    "    if distances is None:\n",
    "        distances = calculate_distmatrix(df)[i]\n",
    "    \n",
    "    if draw_take_all:\n",
    "        d_from0, d_tomax = calculate_d_from0_tomax(distances, K=K, training=training)\n",
    "        zip_distances = list(enumerate(distances))\n",
    "        phidata_list = create_phidata_list(zip_distances, \n",
    "                                           d_from0, d_tomax, i, draw_take_all=draw_take_all, training=training)\n",
    "        return phidata_list\n",
    "    \n",
    "    # pick K nearest distances\n",
    "    indices = list(np.argsort(distances))\n",
    "    phidata_list = [0]*df.shape[0]\n",
    "    j = 0\n",
    "    while j < K:\n",
    "        if indices[j] not in training:\n",
    "            indices.pop(j)\n",
    "        else:\n",
    "            phidata_list[indices[j]] = 1\n",
    "            j+=1\n",
    "    \n",
    "    for i in range(K):\n",
    "        phidata_list[indices[i]] = 1 # first K of them are friends\n",
    "        phidata_list[indices[-1-i]] = 2 # last K of them are enemies\n",
    "    return phidata_list\n",
    "\n",
    "# for debugging\n",
    "# phidata_list = find_distances_and_selected_neighbors(\n",
    "#     total_dataset['monk1'].iloc[:30, :], 0, K = 3, draw_take_all=False, training=each_missing['monk1'])\n",
    "\n",
    "# print()\n",
    "# print(phidata_list)\n",
    "# print(list(zip(phidata_list, distances)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f4db07",
   "metadata": {},
   "source": [
    "<a id='MissingValueCreation'> </a>\n",
    "## Manually create missing values\n",
    "<a href=#TableOfContent> Back to Table of Content </a>\n",
    "\n",
    "For each dataset, we will randomly select 1% of rows/individuals and remove them and 10 of their closest friends too. So that the missing data is always clustered \"close to each other\" centered around the 1% selected rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c93a6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(40)\n",
    "\n",
    "def generate_missing_values(df, perc=0.01, K=10):\n",
    "    \n",
    "    # calculate the amount of 1% selected rows \n",
    "    amount = int(df.shape[0]*perc)\n",
    "    \n",
    "    # select the missing_centers. Create a list storing all the missing rows to be calculated.\n",
    "    missing_center = random.sample(range(df.shape[0]), amount)\n",
    "    missing = []+ missing_center\n",
    "    distmatrix = calculate_distmatrix(df)\n",
    "\n",
    "    # for each missing center, add their 10 closest neighbors\n",
    "    for i in missing_center:\n",
    "        phidata_list = find_distances_and_selected_neighbors(df, i, K=K, training=None, distances=distmatrix[i])\n",
    "        \n",
    "        # for the j-th row with distance d to i. \n",
    "        for (j, d) in enumerate(distmatrix[i]):\n",
    "            if phidata_list[j] == 1: # if j is amongst the K-nearest to the missing center i\n",
    "                missing.append(j)\n",
    "                \n",
    "    # remove the potential duplicates\n",
    "    return list(set(missing))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be9aabb",
   "metadata": {
    "id": "3be9aabb"
   },
   "source": [
    "<a id='WriteDownDataset'> </a>\n",
    "## Write the dataset down\n",
    "<a href=#TableOfContent> Back to Table of Content </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "025b4180",
   "metadata": {
    "id": "025b4180"
   },
   "outputs": [],
   "source": [
    "# create a new directory (if not already existing) called df (stands for dataframe)\n",
    "os.chdir(root)\n",
    "if \"df\" not in os.listdir():\n",
    "    os.mkdir(\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9257422",
   "metadata": {
    "id": "d9257422"
   },
   "outputs": [],
   "source": [
    "# write down data in csv format\n",
    "for (name, df) in total_dataset.items():\n",
    "    df.to_csv(os.path.join(root, \"df\", name+\".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "604afda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value processed for monk1.\n",
      "Missing value processed for monk2.\n",
      "Missing value processed for monk3.\n",
      "Missing value processed for balance_scale_B.\n",
      "Missing value processed for balance_scale_L.\n",
      "Missing value processed for tic_bin.\n",
      "Missing value processed for car_evaluation_vgood.\n",
      "Missing value processed for car_evaluation_good.\n",
      "Missing value processed for car_evaluation_acc.\n",
      "Missing value processed for kr-vs-kp.\n"
     ]
    }
   ],
   "source": [
    "# calculate the missing data for each dataset and write it as json file, \n",
    "# unless it has been written already, in which case we read instead of calculate again.\n",
    "\n",
    "if \"missing_data.json\" in os.listdir(os.path.join(root, \"df\")) and False:\n",
    "    with open(os.path.join(root, \"df\", \"missing_data.json\"), 'r') as f:\n",
    "        each_missing = eval(f.read()) \n",
    "else:\n",
    "    each_missing = dict()\n",
    "    for (name, df) in total_dataset.items():\n",
    "        missing = generate_missing_values(df, perc=0.01, K=10)\n",
    "        each_missing[name] = sorted(missing)\n",
    "        print(f\"Missing value processed for {name}.\")\n",
    "\n",
    "    # write the missing instance down in the folder df.\n",
    "    with open(os.path.join(root, \"df\", \"missing_data.json\"), 'w') as f:\n",
    "        f.write(str(each_missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47a4961f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2292418772563177"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(each_missing['monk1']) / len(total_dataset['monk1']) # about 20 percent missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439e9814",
   "metadata": {
    "id": "439e9814"
   },
   "source": [
    "# Generating data for C++ code\n",
    "Need phi_file, v_predefined_file and w_file\n",
    "<a id='Graphfile'></a>\n",
    "## Graph file, weight file and v predefined file\n",
    "<a href=#TableOfContent> Back to Table of Content </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5504c90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The graph of monk1 has been processed.\n",
      "\n",
      "The graph of monk2 has been processed.\n",
      "\n",
      "The graph of monk3 has been processed.\n",
      "\n",
      "The graph of balance_scale_B has been processed.\n",
      "\n",
      "The graph of balance_scale_L has been processed.\n",
      "\n",
      "The graph of tic_bin has been processed.\n",
      "\n",
      "The graph of car_evaluation_vgood has been processed.\n",
      "\n",
      "The graph of car_evaluation_good has been processed.\n",
      "\n",
      "The graph of car_evaluation_acc has been processed.\n",
      "\n",
      "The graph of kr-vs-kp has been processed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create the graph file, i.e. the adjacency matrix of the (fully-connected) network, where each row is an instance.\n",
    "def create_graphfile(df, filename):\n",
    "    parent = os.path.dirname(filename)\n",
    "    if os.path.basename(filename) in os.listdir(parent):\n",
    "        print(f\"{os.path.basename(filename)} already existed. Finished.\")\n",
    "        return None\n",
    "    graphfile = pd.DataFrame(index=range(df.shape[0]), columns=range(df.shape[0]), dtype=np.int8)\n",
    "    graphfile.iloc[:, :] = 1\n",
    "    np.fill_diagonal(graphfile.values, 0)\n",
    "    graphfile.to_csv(filename, header=False, index=False, sep= ' ', float_format='%.0f')\n",
    "\n",
    "for data in total_dataset.keys():\n",
    "    create_graphfile(total_dataset[data], os.path.join(root, \"data2run\", f\"{data}_graph.txt\"))\n",
    "    print(f\"The graph of {data} has been processed.\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "060ea7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weight of monk1 has been processed.\n",
      "\n",
      "The weight of monk2 has been processed.\n",
      "\n",
      "The weight of monk3 has been processed.\n",
      "\n",
      "The weight of balance_scale_B has been processed.\n",
      "\n",
      "The weight of balance_scale_L has been processed.\n",
      "\n",
      "The weight of tic_bin has been processed.\n",
      "\n",
      "The weight of car_evaluation_vgood has been processed.\n",
      "\n",
      "The weight of car_evaluation_good has been processed.\n",
      "\n",
      "The weight of car_evaluation_acc has been processed.\n",
      "\n",
      "The weight of kr-vs-kp has been processed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create the weight file uniform, i.e. the weighted adjacency matrix of the (fully-connected) network, where each row is an instance.\n",
    "def create_weightfile_uniform(df, filename, overwrite=False):\n",
    "    parent = os.path.dirname(filename)\n",
    "    if not overwrite and os.path.basename(filename) in os.listdir(parent):\n",
    "        print(f\"{os.path.basename(filename)} already existed. Finished.\")\n",
    "        return None\n",
    "    weightfile = pd.DataFrame(index=range(df.shape[0]), columns=range(df.shape[0]), dtype=np.int8)\n",
    "    weightfile.iloc[:, :] = 1\n",
    "    np.fill_diagonal(weightfile.values, 0)\n",
    "    weightfile.to_csv(filename, header=False, index=False, sep= ' ', float_format='%.0f')\n",
    "\n",
    "for data in total_dataset.keys():\n",
    "    create_weightfile_uniform(total_dataset[data], os.path.join(root, \"data2run\", f\"{data}_weight.txt\"))\n",
    "    print(f\"The weight of {data} has been processed.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ab216b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predefined label of monk1 has been processed.\n",
      "\n",
      "The predefined label of monk2 has been processed.\n",
      "\n",
      "The predefined label of monk3 has been processed.\n",
      "\n",
      "The predefined label of balance_scale_B has been processed.\n",
      "\n",
      "The predefined label of balance_scale_L has been processed.\n",
      "\n",
      "The predefined label of tic_bin has been processed.\n",
      "\n",
      "The predefined label of car_evaluation_vgood has been processed.\n",
      "\n",
      "The predefined label of car_evaluation_good has been processed.\n",
      "\n",
      "The predefined label of car_evaluation_acc has been processed.\n",
      "\n",
      "The predefined label of kr-vs-kp has been processed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_v_predefined(df, filename, percentage=0.2, testing=None, overwrite=False): # percentage of test data vs all data\n",
    "    parent = os.path.dirname(filename)\n",
    "    if not overwrite and os.path.basename(filename) in os.listdir(parent):\n",
    "        print(f\"{os.path.basename(filename)} already existed. Finished.\")\n",
    "        return None\n",
    "    \n",
    "    if testing is not None:\n",
    "        # turn indices from testing into -1\n",
    "        v_predefined = (df.iloc[:, 0]).copy() # the labels\n",
    "        for i in testing:\n",
    "            v_predefined[i] = -1\n",
    "        v_predefined.to_csv(filename, header=False, index=False, sep=' ', float_format='%.0f')\n",
    "    else:\n",
    "        a = np.linspace(0, 1, df.shape[0])\n",
    "        a = a < 1-percentage  # create a bool array of length number of rows, beginning with 80% of 1's followed by 20% of 0's\n",
    "        a = pd.Series(a)\n",
    "        a = a.astype(np.int8)\n",
    "        a = (df['label'] * a)-1 + a # turns the beginning 80% into the same as df['label'] and remaining 20% all equals -1\n",
    "        a.to_csv(filename, header=False, index=False, sep=' ', float_format='%.0f')\n",
    "\n",
    "    \n",
    "for data in total_dataset.keys():\n",
    "\n",
    "    v_predefined = create_v_predefined(\n",
    "        total_dataset[data], \n",
    "        os.path.join(root, \"data2run\", f'{data}_v_predefined.txt'),\n",
    "        testing=each_missing[data],\n",
    "        overwrite=True)\n",
    "    print(f\"The predefined label of {data} has been processed.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f67f2a",
   "metadata": {},
   "source": [
    "<a id='Phi'></a>\n",
    "## phi_file \n",
    "<a href=#TableOfContent> Back to Table of Content </a>\n",
    "\n",
    "For telling who are friends and who are enemies. K-nearest are friends and K' farthest are enemies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "446eefa2",
   "metadata": {
    "id": "2b2ca11f",
    "outputId": "82984cb0-9aa4-4840-f6f1-20309913b7bd"
   },
   "outputs": [],
   "source": [
    "# run the find_K_neighbors n times (n = df.shape[0] is the number of rows). Store the lists into a matrix, saved in csv.\n",
    "def create_phifile(df, filename, K=3, training=None, testing=None, overwrite=True):\n",
    "    if testing is not None:\n",
    "        training = [i for i in range(df.shape[0]) if i not in testing]\n",
    "        \n",
    "    parent = os.path.dirname(filename)\n",
    "    if not overwrite and os.path.basename(filename) in os.listdir(parent):\n",
    "        print(f\"{os.path.basename(filename)} already existed. Finished.\")\n",
    "        return None\n",
    "\n",
    "    phifile = pd.DataFrame(index=range(df.shape[0]), columns=range(df.shape[0]))\n",
    "    distmatrix = calculate_distmatrix(df)\n",
    "    for i in range(df.shape[0]):\n",
    "        distances = distmatrix[i]\n",
    "        phidata_list = find_distances_and_selected_neighbors(df, i, K, draw_take_all=False,\n",
    "                                    training=training, distances=distances)\n",
    "        # ignore all the enemies\n",
    "        phidata_list_no_enemies = []\n",
    "        for phi in phidata_list:\n",
    "            if phi == 2:  # correspond to where phidown[i,j]==1\n",
    "                phidata_list_no_enemies.append(0)  # store it as 0, meaning ignoring enemies\n",
    "            else:\n",
    "                phidata_list_no_enemies.append(phi)\n",
    "        phidata_list = phidata_list_no_enemies\n",
    "        \n",
    "        phifile.iloc[i, :] = np.array(phidata_list)\n",
    "\n",
    "\n",
    "    phifile.to_csv(filename, header=False, index=False, sep=' ', float_format='%.0f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b2ca11f",
   "metadata": {
    "id": "2b2ca11f",
    "outputId": "82984cb0-9aa4-4840-f6f1-20309913b7bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing monk1 dataset.\n",
      "Processing monk2 dataset.\n",
      "Processing monk3 dataset.\n",
      "Processing balance_scale_B dataset.\n",
      "Processing balance_scale_L dataset.\n",
      "Processing tic_bin dataset.\n",
      "Processing car_evaluation_vgood dataset.\n",
      "Processing car_evaluation_good dataset.\n",
      "Processing car_evaluation_acc dataset.\n",
      "Processing kr-vs-kp dataset.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for name in total_dataset.keys():\n",
    "    print(f\"Processing {name} dataset.\")\n",
    "    create_phifile(total_dataset[name], \n",
    "                   os.path.join(root, \"data2run\", f\"{name}_phifile.txt\"), \n",
    "                   testing=each_missing[name],\n",
    "                   overwrite=True)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d21549",
   "metadata": {},
   "source": [
    "<a id='Weight'> </a>\n",
    "## Create distance-parametered weight file\n",
    "<a href=#TableOfContent> Back to Table of Content </a>\n",
    "\n",
    "Calculate $d(i, j)$ the distance between i and j. \n",
    "Define for attractive neighbors (where j are one of those closer to i). The closer the distance, the higher the weight. We denote $M = \\lceil median(d(i, :)) \\rceil$\n",
    "$$w(i, j) = M - d(i, j) + 1 \\quad \\mbox{ if } \\quad d(i, j) <= M$$\n",
    "Define for repulsive neighbors (where j are one of those closer to i). The farther the distance, the higher the weight.\n",
    "$$w(i, j) = d(i, j) - M + 1 \\quad \\mbox{ if } \\quad d(i, j) > M$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a8df02a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def weight(distance, median, dmax):\n",
    "    if distance <= median:\n",
    "        return median - distance + 1\n",
    "    else:\n",
    "        return distance - median +1\n",
    "    \n",
    "def calculate_weights_i(df, i, distances=None):\n",
    "    median = distances.median()\n",
    "    weights = distances.copy()\n",
    "    for j in range(df.shape[0]):\n",
    "        if distances[j] <= median:\n",
    "            weights[j] = median -  distances[j] + 1\n",
    "        else:\n",
    "            weights[j] =  distances[j] - median +1\n",
    "    return weights\n",
    "    \n",
    "        \n",
    "        \n",
    "def create_weightfile_distanced(df, filename, overwrite=False):\n",
    "    parent = os.path.dirname(filename)\n",
    "    \n",
    "    if not overwrite and os.path.basename(filename) in os.listdir(parent):\n",
    "        print(f\"{os.path.basename(filename)} already existed. Finished.\")\n",
    "        return None\n",
    "    \n",
    "    weightfile = pd.DataFrame(index=range(df.shape[0]), columns=range(df.shape[0]), dtype=np.int8)\n",
    "    begin=time()\n",
    "    distmatrix = calculate_distmatrix(df)\n",
    "    for i in range(df.shape[0]):\n",
    "        weightfile.iloc[i, :] = calculate_weights_i(df, i, distances=distmatrix[i])\n",
    "\n",
    "    weightfile.to_csv(filename, header=False, index=False, sep= ' ', float_format='%.0f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4efd87f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "Running monk1\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Running monk2\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Running monk3\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Running balance_scale_B\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Running balance_scale_L\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Running tic_bin\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Running car_evaluation_vgood\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Running car_evaluation_good\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Running car_evaluation_acc\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Running kr-vs-kp\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for data in total_dataset.keys():\n",
    "    print(\"-\"*70 + f\"\\nRunning {data}\\n\" + \"-\"*70)\n",
    "    create_weightfile_distanced(total_dataset[data], \n",
    "                            os.path.join(root, \"data2run\", f\"{data}_weightfile_distance.txt\"),\n",
    "                               overwrite=True)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
